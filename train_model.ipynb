{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb0eca41-390a-465f-abdd-607025e484ac",
      "metadata": {
        "id": "cb0eca41-390a-465f-abdd-607025e484ac"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fd5829-b633-4f8c-91b9-18bd7a31dff0",
      "metadata": {
        "id": "78fd5829-b633-4f8c-91b9-18bd7a31dff0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import  StringLookup\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#Prepare data\n",
        "from sklearn.model_selection import train_test_split\n",
        "def load_image(path):\n",
        "\n",
        "    img = tf.io.read_file(\"images/\" + path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_image(img, channels=3)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [50, 200])\n",
        "    # 5. Transpose the image because we want the time\n",
        "    # dimension to correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    img = img.numpy()\n",
        "    return img\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "num_labels = []\n",
        "path_to_works = []\n",
        "data = pd.read_csv(\"labels.csv\")\n",
        "with open(\"vocab.txt\", 'r') as file:\n",
        "    vocab = json.loads(file.read())\n",
        "\n",
        "filenames = data[\"file_name\"]\n",
        "\n",
        "labels = data[\"text\"]\n",
        "\n",
        "characters = set(char for label in labels for char in label)\n",
        "max_length = max([len(label) for label in labels])\n",
        "\n",
        "#merge new and old dictionary\n",
        "for c in characters:\n",
        "    if c not in vocab:\n",
        "        vocab.append(c)\n",
        "\n",
        "with open(\"vocab.txt\", 'w') as file:\n",
        "    file.write(json.dumps(vocab))\n",
        "\n",
        "\n",
        "#save data to several files  \n",
        "for path in filenames[:20000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x1.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[20000:40000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x2.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[40000:60000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x3.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[60000:80000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x4.npz\",a=images, b=np.array([]))\n",
        "\n",
        "\n",
        "for path in filenames[80000:100000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x5.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[100000:120000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x6.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[120000:140000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x7.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[160000:]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x8.npz\",a=images, b=np.array([]))\n",
        "\n",
        "#make all words be equal length\n",
        "def proccess(lbl):\n",
        "\n",
        "    if len(lbl) < max_length:\n",
        "        for i in range(0, max_length - len(lbl)):\n",
        "            lbl+=' '\n",
        "    return lbl\n",
        "labels = list(map(proccess, labels))\n",
        "\n",
        "#tokenize\n",
        "for label in labels:\n",
        "\n",
        "\n",
        "    if label !=\".DS_Store\":\n",
        "        nums = []\n",
        "\n",
        "        for c in label.replace(\".png\",'').replace(\".jpg\",''):\n",
        "            nums.append(vocab.index(c))\n",
        "\n",
        "\n",
        "\n",
        "        num_labels.append(nums)\n",
        "num_labels = np.array(num_labels)\n",
        "np.save(\"labels.npy\", num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d714f2-872c-408c-9b0c-82ef5ac01f99",
      "metadata": {
        "id": "54d714f2-872c-408c-9b0c-82ef5ac01f99"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "x1 = np.load(\"x1.npz\")['a']\n",
        "\n",
        "x2 = np.load(\"x2.npz\")['a']\n",
        "\n",
        "x3 = np.load(\"x3.npz\")['a']\n",
        "\n",
        "x4 = np.load(\"x4.npz\")['a']\n",
        "\n",
        "x5 = np.load(\"x5.npz\")['a']\n",
        "\n",
        "x6 = np.load(\"x6.npz\")['a']\n",
        "\n",
        "x7 = np.load(\"x7.npz\")['a']\n",
        "\n",
        "x8 = np.load(\"x8.npz\")['a']\n",
        "\n",
        "\n",
        "\n",
        "labels = np.load(\"labels.npy\").astype(\"int64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f11aee3-506e-406b-8665-2a6577f3fead",
      "metadata": {
        "id": "0f11aee3-506e-406b-8665-2a6577f3fead"
      },
      "outputs": [],
      "source": [
        "import  tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "import tensorflow.keras\n",
        "import os\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, Bidirectional, LSTM, Reshape, Dropout\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # At test time, just return the computed predictions\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "img_input = Input(shape=(200, 50, 3), name=\"image_input\",dtype=\"float32\" )\n",
        "lbl_input = Input(shape=(None,),dtype=\"float32\")\n",
        "\n",
        "x = layers.Conv2D(64,(3, 3), activation=\"relu\",padding=\"same\")(img_input)\n",
        "\n",
        "x = layers.Conv2D(64,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "\n",
        "x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = layers.Conv2D(128,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "x = layers.Conv2D(128,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "\n",
        "\n",
        "\n",
        "x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = layers.Conv2D(\n",
        "        64,\n",
        "        (3, 3),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv1\",\n",
        "    )(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "x = Reshape(((200 // 4), (50 // 4) * 64))(x)\n",
        "\n",
        "x = Dense(64, activation=\"relu\",kernel_initializer=\"he_normal\")(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.35))(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.35), name=\"last_lstm\")(x)\n",
        "\n",
        "x = Dense(151, activation=\"softmax\", name=\"target_dense\")(x)\n",
        "output = CTCLayer()(lbl_input, x)\n",
        "\n",
        "model = Model([img_input, lbl_input], output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CERMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    A custom Keras metric to compute the Character Error Rate\n",
        "    \"\"\"\n",
        "    def __init__(self, name='CER_metric', **kwargs):\n",
        "        super(CERMetric, self).__init__(name=name, **kwargs)\n",
        "        self.cer_accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
        "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        input_shape = K.shape(y_pred)\n",
        "        input_length = tf.ones(shape=input_shape[0]) * K.cast(input_shape[1], 'float32')\n",
        "\n",
        "        decode, log = K.ctc_decode(y_pred,\n",
        "                                    input_length,\n",
        "                                    greedy=True)\n",
        "\n",
        "        decode = K.ctc_label_dense_to_sparse(decode[0], K.cast(input_length, 'int32'))\n",
        "        y_true_sparse = K.ctc_label_dense_to_sparse(y_true, K.cast(input_length, 'int32'))\n",
        "\n",
        "        decode = tf.sparse.retain(decode, tf.not_equal(decode.values, -1))\n",
        "        distance = tf.edit_distance(decode, y_true_sparse, normalize=True)\n",
        "\n",
        "        self.cer_accumulator.assign_add(tf.reduce_sum(distance))\n",
        "        self.counter.assign_add(tf.cast(len(y_true),tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.cer_accumulator, self.counter)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.cer_accumulator.assign(0.0)\n",
        "        self.counter.assign(0.0)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2,monitor=\"val_CER_metric\",mode=\"min\", restore_best_weights=True)\n",
        "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_CER_metric', factor=0.2,patience=2,verbose=1, min_delta=0.006,mode=\"min\", min_lr=0.0001)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"result.h5\", monitor=\"val_CER_metric\",mode='min', save_best_only=True, save_weights_only=True)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[CERMetric()])\n",
        "\n",
        "model.fit([x_train,y_train],y_train, epochs=30, batch_size=512, validation_split=0.05, callbacks=[early_stopping,model_checkpoint,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2233bf9c-9daa-47e0-96f4-d97826b3d2a8",
      "metadata": {
        "id": "2233bf9c-9daa-47e0-96f4-d97826b3d2a8"
      },
      "outputs": [],
      "source": [
        "model.evaluate([x_test,y_test], y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}